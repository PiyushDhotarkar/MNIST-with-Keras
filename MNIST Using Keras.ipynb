{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.datasets import mnist \n",
    "import seaborn as sns\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation \n",
    "import keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the data directly to train and test\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of training images 60000  Dimensions: 28 * 28\n",
      "Total Number of testing images 10000  Dimensions: 28 * 28\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of training images\", X_train.shape[0], \" Dimensions:\", X_train.shape[1], '*', X_train.shape[2])\n",
    "print('Total Number of testing images', X_test.shape[0], \" Dimensions:\", X_test.shape[1], '*', X_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the 28*28 pixel image in a 1D vector of 1*784\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  13  25 100 122   7   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  33 151 208 252 252 252 146   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  40 152 244 252 253 224 211 252 232\n",
      "  40   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  15\n",
      " 152 239 252 252 252 216  31  37 252 252  60   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  96 252 252 252 252 217  29   0  37\n",
      " 252 252  60   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 181 252 252 220 167  30   0   0  77 252 252  60   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  26 128  58  22   0   0   0\n",
      "   0 100 252 252  60   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 157 252 252  60   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 110\n",
      " 121 122 121 202 252 194   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  10  53 179 253 253 255 253 253 228  35   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5  54 227 252\n",
      " 243 228 170 242 252 252 231 117   6   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   6  78 252 252 125  59   0  18 208 252 252 252 252\n",
      "  87   7   0   0   0   0   0   0   0   0   0   0   0   0   5 135 252 252\n",
      " 180  16   0  21 203 253 247 129 173 252 252 184  66  49  49   0   0   0\n",
      "   0   0   0   0   0   3 136 252 241 106  17   0  53 200 252 216  65   0\n",
      "  14  72 163 241 252 252 223   0   0   0   0   0   0   0   0 105 252 242\n",
      "  88  18  73 170 244 252 126  29   0   0   0   0   0  89 180 180  37   0\n",
      "   0   0   0   0   0   0   0 231 252 245 205 216 252 252 252 124   3   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 207\n",
      " 252 252 252 252 178 116  36   4   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  13  93 143 121  23   6   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Normalization\n",
    "X_train = X_train/255  ## As the value ranges from 0 to 255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the target variable into a 10D vector/ Like a one hot encoding for the target variable\n",
    "Y_train = utils.to_categorical(Y_train,10)\n",
    "Y_test = utils.to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Parmaters\n",
    "output = 10\n",
    "input_dim = X_train.shape[1]\n",
    "batch = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: - MLP with Sigmoid activations and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.5458 - acc: 0.8593 - val_loss: 0.2542 - val_acc: 0.9273\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2221 - acc: 0.9352 - val_loss: 0.1829 - val_acc: 0.9434\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.1645 - acc: 0.9512 - val_loss: 0.1426 - val_acc: 0.9575\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1261 - acc: 0.9626 - val_loss: 0.1183 - val_acc: 0.9644\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0987 - acc: 0.9709 - val_loss: 0.1057 - val_acc: 0.9680\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0791 - acc: 0.9771 - val_loss: 0.0946 - val_acc: 0.9716\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0642 - acc: 0.9811 - val_loss: 0.0873 - val_acc: 0.9720\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0514 - acc: 0.9847 - val_loss: 0.0753 - val_acc: 0.9761\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0415 - acc: 0.9879 - val_loss: 0.0722 - val_acc: 0.9780\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0346 - acc: 0.9897 - val_loss: 0.0709 - val_acc: 0.9774\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0271 - acc: 0.9926 - val_loss: 0.0678 - val_acc: 0.9795\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0222 - acc: 0.9941 - val_loss: 0.0645 - val_acc: 0.9809\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0182 - acc: 0.9955 - val_loss: 0.0737 - val_acc: 0.9789\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0144 - acc: 0.9964 - val_loss: 0.0656 - val_acc: 0.9818\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0115 - acc: 0.9972 - val_loss: 0.0736 - val_acc: 0.9798\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0757 - val_acc: 0.9779\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.0763 - val_acc: 0.9780\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0064 - acc: 0.9987 - val_loss: 0.0717 - val_acc: 0.9815\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.0657 - val_acc: 0.9825\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.0748 - val_acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "## Building an MLP with Sigmoid activation using Adam Optimizer\n",
    "model_sig = Sequential()\n",
    "model_sig.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\n",
    "model_sig.add(Dense(128, activation='sigmoid'))\n",
    "model_sig.add(Dense(output, activation = 'softmax'))\n",
    "\n",
    "model_sig.summary()\n",
    "model_sig.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_sig.fit(X_train, Y_train, batch_size=batch, epochs=epochs, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.07478052209437083  Test Accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "## Model Evaluation\n",
    "prediction = model_sig.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Loss:\", prediction[0],  \" Test Accuracy:\", prediction[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: - MLP With Relu activations and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2185 - acc: 0.9343 - val_loss: 0.1026 - val_acc: 0.9664\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0843 - acc: 0.9744 - val_loss: 0.0888 - val_acc: 0.9725\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0507 - acc: 0.9844 - val_loss: 0.0765 - val_acc: 0.9755\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0362 - acc: 0.9888 - val_loss: 0.0719 - val_acc: 0.9763\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0261 - acc: 0.9922 - val_loss: 0.0707 - val_acc: 0.9783\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0211 - acc: 0.9931 - val_loss: 0.0740 - val_acc: 0.9789\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0914 - val_acc: 0.9757\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0904 - val_acc: 0.9776\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0146 - acc: 0.9949 - val_loss: 0.0805 - val_acc: 0.9789\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0141 - acc: 0.9951 - val_loss: 0.1188 - val_acc: 0.9706\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 58us/sample - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0952 - val_acc: 0.9788\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0838 - val_acc: 0.9803\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0884 - val_acc: 0.9797\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0901 - val_acc: 0.9790\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0075 - acc: 0.9973 - val_loss: 0.0832 - val_acc: 0.9813\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0978 - val_acc: 0.9790\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0913 - val_acc: 0.9805\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.1025 - val_acc: 0.9798\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0073 - acc: 0.9977 - val_loss: 0.0932 - val_acc: 0.9807\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0902 - val_acc: 0.9823\n"
     ]
    }
   ],
   "source": [
    "## Model 2:- MLP with relu activations using adam optimizer\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n",
    "model_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n",
    "model_relu.add(Dense(output, activation='softmax'))\n",
    "\n",
    "print(model_relu.summary())\n",
    "\n",
    "model_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_relu.fit(X_train, Y_train, batch_size=batch, epochs=epochs, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.09018854618649179  Test Accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "## Model Evaluation\n",
    "prediction = model_relu.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Loss:\", prediction[0],  \" Test Accuracy:\", prediction[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3:- MLP with Sigmoid activations and Dropout Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.9524 - acc: 0.6986 - val_loss: 0.3105 - val_acc: 0.9103\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4276 - acc: 0.8752 - val_loss: 0.2340 - val_acc: 0.9291\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3295 - acc: 0.9037 - val_loss: 0.1899 - val_acc: 0.9418\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2784 - acc: 0.9189 - val_loss: 0.1630 - val_acc: 0.9498\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.2425 - acc: 0.9287 - val_loss: 0.1444 - val_acc: 0.9541\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2146 - acc: 0.9377 - val_loss: 0.1301 - val_acc: 0.9591\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.1926 - acc: 0.9434 - val_loss: 0.1151 - val_acc: 0.9642\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.1758 - acc: 0.9488 - val_loss: 0.1048 - val_acc: 0.9686\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.1594 - acc: 0.9528 - val_loss: 0.1022 - val_acc: 0.9693\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.1494 - acc: 0.9560 - val_loss: 0.0957 - val_acc: 0.9707\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.1367 - acc: 0.9598 - val_loss: 0.0869 - val_acc: 0.9731\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.1266 - acc: 0.9627 - val_loss: 0.0836 - val_acc: 0.9745\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.1195 - acc: 0.9649 - val_loss: 0.0826 - val_acc: 0.9751\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.1126 - acc: 0.9664 - val_loss: 0.0785 - val_acc: 0.9770\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.1059 - acc: 0.9682 - val_loss: 0.0767 - val_acc: 0.9768\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.1022 - acc: 0.9691 - val_loss: 0.0745 - val_acc: 0.9769\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0946 - acc: 0.9719 - val_loss: 0.0726 - val_acc: 0.9778\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0913 - acc: 0.9726 - val_loss: 0.0704 - val_acc: 0.9796\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0861 - acc: 0.9740 - val_loss: 0.0704 - val_acc: 0.9782\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0844 - acc: 0.9735 - val_loss: 0.0674 - val_acc: 0.9802\n"
     ]
    }
   ],
   "source": [
    "## Model 3: - with sigmoid activations and some dropout layers\n",
    "\n",
    "model_dropout = Sequential()\n",
    "\n",
    "model_dropout.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\n",
    "model_dropout.add(Dropout(0.5))\n",
    "\n",
    "model_dropout.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\n",
    "model_dropout.add(Dropout(0.5))\n",
    "\n",
    "model_dropout.add(Dense(output, activation='softmax'))\n",
    "\n",
    "model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_dropout.fit(X_train, Y_train, batch_size=batch, epochs=epochs, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.06742319069547811\n",
      "Test accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "## Model evaluation\n",
    "prediction = model_dropout.evaluate(X_test, Y_test, verbose=0) \n",
    "print('Test loss:', prediction[0]) \n",
    "print('Test accuracy:', prediction[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
